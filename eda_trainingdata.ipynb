import pandas as pd
import re
from unidecode import unidecode
import numpy as np
import scipy
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import seaborn as sns

train_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Dataset/train_preprocess.tsv', sep = '\t', names=['sentences', 'stat'])

train_df.shape

## Dealing with duplicates

#Total of duplications
train_df.duplicated().sum()

#Locating duplicates
train_df.loc[train_df.duplicated(), :]

#Checking duplicate sample
train_df[train_df['sentences'] == 'tidak indah']

#Deleting duplicates
train_df = train_df.drop_duplicates(keep='last')

## Punctuations, newlines, multi-spaces deletion

#Wipe out punctuations
def remove_punctuation(s):
    s = re.sub(r"\\x[A-Za-z0-9./]+", '', unidecode(s))
    return re.sub(r"[^\w\d\s]+","",s)

train_df['sentences_1'] = train_df['sentences'].apply(remove_punctuation)

#Wipe out new line
def remove_newline(s):
    return s.strip().replace(r'\n'," ")
    
train_df['sentences_1'] = train_df['sentences_1'].apply(remove_newline)

#Wipe out multi-spaces
def remove_spaces(s):
    return  re.sub(' +', ' ',s)
    
train_df['sentences_1'] = train_df['sentences_1'].apply(remove_spaces)

#Wipe out Indonesian stopwords -> sw stopwords
## sw_id = pd.read_csv (r'id_stopwords.txt')
## sw_id.to_csv (r'id_stopwords.csv', index=None)
def stopwords_remove(s):
    sw_id = pd.read_csv ('/content/drive/MyDrive/Colab Notebooks/Dataset/id_stopwords.csv', names=['sw'])
    list_sw = sw_id['sw'].to_list()
    s = s.split(" ") # merubah string menjadi list
    s = [x for x in s if x not in list_sw] # menghapus kata pada s, jika kata tersebut ada pada list_sw
    s = ' '.join(s) # menggabung list menjadi string, dipisah oleh spasi
    return s

train_df['sentences_1'] = train_df['sentences_1'].apply(stopwords_remove)
train_df.head()

# Stemming -> turns words into it's origin
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
def indo_stemming(s):
    factory = StemmerFactory()
    stemmer = factory.create_stemmer()
    return stemmer.stem(s)

train_df['sentences_1'] = train_df['sentences_1'].apply(indo_stemming)
train_df.head()

# Uploading kamusalay for normalization
kamus = pd.read_csv('/content/new_kamusalay.csv', names = ['sebelum', 'sesudah'], encoding='latin-1')

# Words normalization -> turns words into a normal condition
def _normalization(s):
  global number
  words = s.split()
  clear_words = ""
  for val in words:
    x = 0
    for idx, data in enumerate(kamus['sebelum']):
      if(val == data):
        clear_words += kamus['sesudah'][idx] + ' '
        print(number,"Transform :",data,"-",kamus['sesudah'][idx])
        x = 1
        number += 1
        break
    if(x == 0):
      clear_words += val + ' '
  return clear_words

train_df['sentences_1'] = train_df['sentences_1'].apply(_normalization)

# Dropping unecessary feature and re-order the feature position
train_df = train_df.drop('sentences', axis=1)
train_df = train_df[['sentences_1','stat']]

# Counting words in every row -> creating new feature/column 
def split_kalimat(text):
  return (len(text.split()))

train_df['total_kata'] = train_df['sentences_1'].apply(split_kalimat)

def remove_nya(text):
  return re.sub('nya', '', text)

train_df['sentences_1'] = train_df['sentences_1'].apply(remove_nya)

## EXPLORATORY DATA ANALYSIS

# Plotting data based on their label/status (positive, neutral, negative)
sns.catplot(data=train_df, x="stat", kind="count", palette="Set2").set(title='Reviews by Its Label')

# Wordcloud distinct words -> using collocation
text = ' '.join(train_df['sentences_1']) #menggabungkan semua text dalam dataframe, menjadi satu string
wordcloud = WordCloud(collocations=False, max_font_size=50, background_color="white").generate(text)

plt.imshow(wordcloud, interpolation="bicubic")
plt.axis('off')
plt.show()

# Wordcloud distinct words -> 10 most used words in the dataset
text = ' '.join(train_df['sentences_1']) #menggabungkan semua text dalam dataframe, menjadi satu string
wordcloud = WordCloud(collocations=False, max_font_size=50, max_words=10, background_color="white").generate(text)

plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()

# Counting Word Frequency to count what words written the most 
word_freq = train_df['sentences_1'].str.split(expand=True).stack().value_counts()
word_freq = pd.DataFrame(word_freq)
word_freq.to_csv('/content/drive/MyDrive/Colab Notebooks/Dataset/word_freq.csv', index=True)
# CSV file downloaded and edited to give column name

word_freq = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Dataset/word_freq.csv")
wf = word_freq.head(10)

# Plotting 10 most used words on bar plot
sns.catplot(data=wf, x="jumlah", y="kata", kind="bar").set(title='10 Most Written Words')

# Counting the frequency of abusive words occured in the dataset (using abusive.csv file)
abusive = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Dataset/abusive.csv")

# Merging word frequency and abusive words
df_merge = pd.merge(
    left=word_freq,
    right=abusive,
    left_on='kata',
    right_on='ABUSIVE',
    how='left'
)
filtered_df = df_merge[df_merge['ABUSIVE'].notnull()]
filtered_df = filtered_df.drop(['ABUSIVE'], axis=1)

abusive10 = filtered_df.head(10) # -> take only 10 most used abusive words

# Plotting into bar plot
sns.catplot(data=abusive10, x="jumlah", y="kata", kind="bar").set(title='10 Most Written Abusive Words')

# Categorize total_kata into 5 classes
def func(x):
    if 0 < x <= 10:
        return '0-10'
    elif 10 < x <= 20:
        return '11-20'
    elif 20 < x <= 30:
        return '21-30'
    elif 30 < x <= 40:
        return '31-40'        
    return '>40'

train_df['sum_category'] = train_df['total_kata'].apply(func)
train_df.head()

# Deleting reviews with 0 word
train_df2 = train_df.loc[(train_df["total_kata"] != 0)]

# Plotting 
sns.catplot(data=train_df2, x="sum_category", kind="count", palette="Set2").set(title='Reviews by Total Words Category')

# Plotting distribution based on their label
sns.catplot(
    data=train_df2, x="total_kata", y="stat",
    kind="box", dodge=False,
).set(title='Distributions Based on Labels')

